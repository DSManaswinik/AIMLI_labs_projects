{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdDik1iSX3UVC5Dm4BKFdw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DSManaswinik/AIMLI_labs_projects/blob/main/assignment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzFcHi4hr3GA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Experimenting with Different Transformation Matrices\n",
        "Transforming features involves using transformation matrices to manipulate the input features, such as scaling, rotating, or applying a custom transformation that could improve the model’s performance. To experiment with different transformation matrices, you could:\n",
        "\n",
        "Apply simple transformations such as normalization, standardization, or scaling.\n",
        "Use domain-specific transformations such as Fourier or wavelet transforms for time-series data.\n",
        "Experiment with dimensionality reduction techniques like Principal Component Analysis (PCA) or feature selection.\n",
        "Answer\n",
        "After experimenting with different transformation matrices, you may find that specific transformations work better for certain features depending on the nature of the dataset. For instance, scaling may improve linear models, while PCA can help in reducing noise.\n",
        "\n",
        "2. Will the Same Transformation Work for All Features?\n",
        "Not necessarily. Features in your dataset may represent different types of data (e.g., categorical, numerical, or ordinal). A single transformation may not optimize all features:\n",
        "\n",
        "Continuous features might benefit from scaling or normalization.\n",
        "Categorical features might require one-hot encoding or embeddings.\n",
        "Specific domain features (e.g., images) might need advanced transformations (e.g., convolution filters).\n",
        "Answer\n",
        "The same transformation is unlikely to work universally for all features. It’s often better to preprocess each feature differently based on its type and importance to the model.\n",
        "\n",
        "3. Is Adding All Features the Best Strategy?\n",
        "Adding all features at once might not always yield the best results. The reasons could be:\n",
        "\n",
        "Multicollinearity: Some features might be highly correlated, leading to redundancy.\n",
        "Curse of Dimensionality: Adding too many irrelevant features can degrade model performance.\n",
        "Overfitting: Including all features may increase the risk of overfitting, especially if the dataset is small.\n",
        "Answer\n",
        "Adding all features at once may not be optimal. Instead, you can:\n",
        "\n",
        "Perform feature selection: Use techniques like recursive feature elimination, mutual information scores, or feature importance from models.\n",
        "Try combinations of features: For example, train the model with 2 or 3 features at a time and evaluate its accuracy. You might find that specific combinations lead to better generalization.\n",
        "Better Combination of Features\n",
        "To find the best combination of features:\n",
        "\n",
        "Start with pairs of features and calculate the model accuracy for each pair.\n",
        "Gradually add features based on their contribution to improving accuracy.\n",
        "Use cross-validation to evaluate feature subsets.\n",
        "Visualize feature importance using techniques like SHAP or permutation importance.\n",
        "Answer\n",
        "By trying different combinations, you might discover that a subset of features (e.g., {Feature1, Feature3}) yields the best results. This could happen because these features capture the most relevant information without redundancy or noise.\n",
        "\n"
      ],
      "metadata": {
        "id": "o8F02TDSr7YX"
      }
    }
  ]
}