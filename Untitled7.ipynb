{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCRgcXO/OyamcWmk/CQMP1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DSManaswinik/AIMLI_labs_projects/blob/main/Untitled7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEkZc1SPF3GV"
      },
      "outputs": [],
      "source": [
        "\n",
        "Module 2: Appreciating, Interpreting and Visualizing Data\n",
        "Lab 3: Manifold Learning Methods\n",
        "Today, we will be focussing on non-linear dimensionality reduction methods or Manifold learning methods.\n",
        "\n",
        "So a manifold is any space that is locally Euclidean. For example, the Earth is round but it looks flat to us. The Earth is a manifold: locally it is flat, but globally we know it is a sphere. Then, manifold learning performs dimensionality reduction by representing data as low-dimensional manifolds embedded in a higher-dimensional space.\n",
        "\n",
        "We often suspect that high-dim may actually lie on or near a low-dim manifold (often much lower!) and it would be useful if we could reparametrize the data in terms of this manifold, yielding a low-dim embedding BUT - we typically donâ€™t know the form of this manifold.\n",
        "\n",
        "image.png\n",
        "\n",
        "ISOMAP\n",
        "Isomap stands for ISOmetric feature MAPping. Isomap is a non-linear dimensionality reduction method based on the spectral theory which tries to preserve the geodesic distances in the lower dimension.\n",
        "\n",
        "But what are Geodesic Distances?\n",
        "image.png\n",
        "\n",
        "The next question should be:\n",
        "\n",
        "How can we compute geodesics without knowing the manifold? ....... ANSWER: So we build an adjacency graph and approximate geodesic distances by shortest-paths through the graph.\n",
        "\n",
        "image.png\n",
        "\n",
        "3 steps for ISOMAP:\n",
        "Build the adjacency graph over the points using a Neighbourhood selection method (let's say k nearest neighbours)\n",
        "Compute approximate geodesics: Weight graph edges by inter-point distances and then apply Dijkstraâ€™s all-pairs shortest-paths algorithm.\n",
        "Take the top d eigenvectors of the Gram matrix.\n",
        "\n",
        "[ ]\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn import datasets\n",
        "First let us simulate a dataset with the points lying on a manifold which we would want our Isomap to be able to capture.\n",
        "\n",
        "We will be using the make_s_curve() function in the sklearn.datasets module to make the manifold using 1000 data points\n",
        "\n",
        "\n",
        "[ ]\n",
        "n_points = 1000\n",
        "X, color = datasets.make_s_curve(n_points, random_state=0)\n",
        "\n",
        "[ ]\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.Dark2)\n",
        "ax.view_init(4, -72)\n",
        "\n",
        "Note that all the points appear to be a lying on a curved 2d plane in this 3d dataset. Now we will be coding the Isomap algorithm to perform this manifold learning.\n",
        "\n",
        "Step 1a: First lets compute the distance matrix (pairwise Euclidean distances) from the data\n",
        "\n",
        "[ ]\n",
        "def dist(a, b):\n",
        "  '''Computes Euclidean distance between 2 points'''\n",
        "  return np.sqrt(sum((a - b) ** 2))\n",
        "\n",
        "distances = np.array([[dist(p1, p2) for p2 in X] for p1 in X])\n",
        "\n",
        "# For N points in the dataset, this matrix should be of the dimension NxN.\n",
        "# Our dataset had 1000 points, so we expect the dimensions of this matrix to be 1000x1000\n",
        "print(distances.shape)\n",
        "(1000, 1000)\n",
        "Step 1b: Let us keep only k nearest neighbors for each point in the distance matrix.\n",
        "\n",
        "[ ]\n",
        "# Lets keep only the 10 nearest neighbors, others set to 0 (= unreachable)\n",
        "\n",
        "n_neighbors = 10\n",
        "neighbors = np.zeros(distances.shape)\n",
        "\n",
        "sort_distances = np.argsort(distances, axis=1)[:, 1:n_neighbors+1]\n",
        "for k,i in enumerate(sort_distances):\n",
        "    neighbors[k,i] = distances[k,i]\n",
        "\n",
        "[ ]\n",
        "neighbors.shape\n",
        "(1000, 1000)\n",
        "Step 2: Weight graph edges by inter-point distances and then apply Dijkstraâ€™s all-pairs shortest-paths algorithm.\n",
        "We will be using the graph_shortest_path() function in the sklearn.utils.graph module.\n",
        "\n",
        "The function takes an array of positive distances as the parameter. It then performs a shortest-path graph search on the graph and returns a matrix G (shape = N,N), where, G(i,j) gives the shortest distance from point i to point j along the graph.\n",
        "\n",
        "\n",
        "[ ]\n",
        "from scipy.sparse.csgraph import shortest_path\n",
        "\n",
        "graph = shortest_path(neighbors, directed=False)\n",
        "Step 3: Take the top d eigenvectors of the Gram matrix.\n",
        "So let us first compute the gram matrix. If we have a matrix graph, then its Gram matrix is graph.T * graph\n",
        "\n",
        "\n",
        "[ ]\n",
        "gram = (graph ** 2)\n",
        "We will now be computing the eigenvectors for this matrix, so lets first center the data points.\n",
        "\n",
        "\n",
        "[ ]\n",
        "n_samples = gram.shape[0]\n",
        "\n",
        "# Mean for each row/column\n",
        "meanrows = np.sum(gram, axis=0) / n_samples\n",
        "meancols = (np.sum(gram, axis=1)/n_samples)[:, np.newaxis]\n",
        "\n",
        "# NUMPY TRICKS: np.newaxis is used to increase the dimension of the existing array by one more dimension. So a (1000,) matrix becomes a (1000,1) matrix.\n",
        "\n",
        "# Mean across all rows (entire matrix)\n",
        "meanall = meanrows.sum() / n_samples\n",
        "\n",
        "gram -= meanrows\n",
        "gram -= meancols\n",
        "gram += meanall\n",
        "Computing eigenvectors\n",
        "\n",
        "\n",
        "[ ]\n",
        "# Using the np.linalg.eig() to compute eigenvectors and eigenvalues for the matrix\n",
        "eig_val_cov, eig_vec_cov = np.linalg.eig(gram)\n",
        "\n",
        "# We will be sorting the eigenvalues and to preserve the corresponding eigenvectors, let us make a list of (eigenvalue, eigenvector) tuples\n",
        "eig_pairs = [(np.abs(eig_val_cov[i]), eig_vec_cov[:, i]) for i in range(len(eig_val_cov))]\n",
        "\n",
        "# We want to reduce the dimensionality to **n_components** dimensions, therefore we will be first taking the top **n_components** eigenvectors (sorted in descending by eigenvalues)\n",
        "eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
        "eig_pa_chebyshevirs = np.array(eig_pairs, dtype=object)\n",
        "\n",
        "# Transforming the matrix (dimensionality reduction)\n",
        "n_components = 2\n",
        "\n",
        "reduced_matrix = np.zeros((n_components, gram.shape[1]))\n",
        "for i in range(n_components):\n",
        "  reduced_matrix[i,:] = eig_pa_chebyshevirs[i,1]\n",
        "reduced_matrix = reduced_matrix.T\n",
        "Plotting the resultant reduced_matrix\n",
        "\n",
        "[ ]\n",
        "ax = fig.add_subplot(111)\n",
        "plt.scatter(reduced_matrix[:, 0], reduced_matrix[:, 1], c=color, cmap=plt.cm.Dark2)\n",
        "plt.title(\"Isomap on S curve\")\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "As you can see the Isomap was able to capture the underlying 2d manifold in the S curve dataset.\n",
        "Now of course there is an easier way to do this, using a predefined library Isomap by sklearn in the manifold module. It just requires you to enter the number of neighbours to be considered and the number of components the graph is to be reduced to.\n",
        "\n",
        "\n",
        "[ ]\n",
        "from sklearn.manifold import Isomap\n",
        "\n",
        "[ ]\n",
        "Y = Isomap(n_neighbors=10, n_components=2).fit_transform(X)\n",
        "\n",
        "ax = fig.add_subplot(111)\n",
        "plt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Dark2)\n",
        "plt.title(\"Isomap on S curve\")\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "Now let us see if Isomap can learn the manifold representation in some more complex datasets\n",
        "\n",
        "[ ]\n",
        "from sklearn.datasets import fetch_lfw_people\n",
        "faces = fetch_lfw_people(min_faces_per_person=200)\n",
        "faces.data.shape\n",
        "(766, 2914)\n",
        "We loaded 766 images, each having 2914 pixels. Let us visualize our dataset before reducing the dimension\n",
        "\n",
        "\n",
        "[ ]\n",
        "fig, ax = plt.subplots(4, 8, subplot_kw=dict(xticks=[], yticks=[]))\n",
        "for i, axi in enumerate(ax.flat):\n",
        "    axi.imshow(faces.images[i], cmap='gray')\n",
        "\n",
        "Our data is 2914 dimensional and our goal is to learn a low dimensional manifold from it. We first apply PCA (which we learn in Lab 2) and see if PCA is able to reduce it to lower dimensions while preserving much of the variance.\n",
        "\n",
        "\n",
        "[ ]\n",
        "from sklearn.decomposition import PCA as RandomizedPCA\n",
        "model = RandomizedPCA(100).fit(faces.data)\n",
        "plt.plot(np.cumsum(model.explained_variance_ratio_))\n",
        "plt.xlabel('n components')\n",
        "plt.ylabel('cumulative variance')\n",
        "plt.plot(range(100), 0.9*np.ones(100))\n",
        "\n",
        "So we can clearly observe that PCA requires more than 70 dimensions to be able to explain 90% of the variance, implying that PCA is failing to learn the underlying manifold effectively.\n",
        "\n",
        "Let us now try to apply Isomap to this dataset and see if it is able to learn the representation effectively.\n",
        "\n",
        "\n",
        "[ ]\n",
        "model = Isomap(n_components=2, n_neighbors = 5)\n",
        "proj = model.fit_transform(faces.data)\n",
        "proj.shape\n",
        "(766, 2)\n",
        "We have reduced the dataset from 2914 dimensions to just 2 dimensions. Let us now visualize the result to be able to better understand the latent representation learnt by Isomap\n",
        "\n",
        "\n",
        "[ ]\n",
        "from matplotlib import offsetbox\n",
        "\n",
        "def plot_components(data, model, images=None, ax=None, thumb_frac=0.05, cmap='gray'):\n",
        "\n",
        "    ax = ax or plt.gca()\n",
        "    proj = model.fit_transform(data)\n",
        "    ax.plot(proj[:, 0], proj[:, 1], '.k')\n",
        "\n",
        "    if images is not None:\n",
        "        min_dist_2 = (thumb_frac * max(proj.max(0) - proj.min(0))) ** 2\n",
        "        shown_images = np.array([2 * proj.max(0)])\n",
        "        for i in range(data.shape[0]):\n",
        "            dist = np.sum((proj[i] - shown_images) ** 2, 1)\n",
        "            if np.min(dist) < min_dist_2:\n",
        "                # don't show points that are too close\n",
        "                continue\n",
        "            shown_images = np.vstack([shown_images, proj[i]])\n",
        "            imagebox = offsetbox.AnnotationBbox(\n",
        "                offsetbox.OffsetImage(images[i], cmap=cmap),\n",
        "                                      proj[i])\n",
        "            ax.add_artist(imagebox)\n",
        "\n",
        "[ ]\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "plot_components(faces.data,\n",
        "                model=Isomap(n_components=2, n_neighbors = 5),\n",
        "                images=faces.images[:, ::2, ::2])\n",
        "plt.xlabel(\"Face Orientation\")\n",
        "plt.ylabel(\"Image Darkness\")\n",
        "\n",
        "We get a very interesting result here. You can see that the Isomap was able to capture very interesting insights regarding the dataset.\n",
        "\n",
        "If you observe along the x-axis, the images are positioned such that the orientation of the faces changes from left to right. Similarly, the darkness of the image increases in the positive direction of the y-axis.\n",
        "\n",
        "This clearly indicates that Isomap was able to capture the underlying manifold very precisely in just 2 dimensions.\n",
        "\n",
        "You are motivated to play around with the number of neighbors and check if the following holds true across neighbourhoods!\n",
        "\n",
        "Exercises!!\n",
        "How do you think would the number of neighbors effect the Isomap algorithm? What happens when the number of neighbors considered is very large? What happens when it is very low?\n",
        "\n",
        "When is the ISOMAP algorithm superior to PCA?\n",
        "\n",
        "Search up another manifold learning methods other than ISOMAP? If there are any, whats the key difference ?\n",
        "\n",
        "Suggest ways to deal with missing data in manifold learning.\n",
        "\n",
        "Conclusion:\n",
        "In practice manifold learning techniques tend to be finicky enough that they are rarely used for anything more than simple qualitative visualization of high-dimensional data.\n",
        "\n",
        "The following are some of the particular challenges of manifold learning, which all contrast poorly with PCA:\n",
        "\n",
        "1) In manifold learning, there is no good framework for handling missing data. In contrast, there are straightforward iterative approaches for missing data in PCA.\n",
        "\n",
        "2) In manifold learning, the presence of noise in the data can \"short-circuit\" the manifold and drastically change the embedding. In contrast, PCA naturally filters noise from the most important components.\n",
        "\n",
        "3) The manifold embedding result is generally highly dependent on the number of neighbors chosen, and there is generally no solid quantitative way to choose an optimal number of neighbors. In contrast, PCA does not involve such a choice.\n",
        "\n",
        "4) In manifold learning, the globally optimal number of output dimensions is difficult to determine. In contrast, PCA lets you find the output dimension based on the explained variance.\n",
        "\n",
        "5) In manifold learning, the meaning of the embedded dimensions is not always clear. In PCA, the principal components have a very clear meaning.\n",
        "\n",
        "6) In manifold learning the computational expense of manifold methods scales as O[N^2] or O[N^3]. For PCA, there exist randomized approaches that are generally much faster (though see the megaman package for some more scalable implementations of manifold learning).\n",
        "\n",
        "Therefore with all that on the table, the only clear advantage of manifold learning methods over PCA is their ability to preserve nonlinear relationships in the data; for that reason I tend to explore data with manifold methods only after first exploring them with PCA.\n",
        "\n",
        "impact of the Number of Neighbors on Isomap The number of neighbors ( ð‘˜ k) in the Isomap algorithm plays a critical role in how the manifold is constructed:\n",
        "\n",
        "When ð‘˜ k is Very Large:\n",
        "\n",
        "The algorithm connects almost every point to many others, approximating a fully connected graph. The manifold may lose its local structure, as distances between points can be overly smoothed, reducing sensitivity to non-linear relationships. This can lead to a poor representation of the true low-dimensional manifold. When ð‘˜ k is Very Small:\n",
        "\n",
        "Only the closest points are connected, and the graph becomes sparse. If ð‘˜ k is too small, the graph may become disconnected, causing the algorithm to fail in preserving global structure. The manifold reconstruction may overfit to noise or local variations. Optimal ð‘˜ k: Typically, ð‘˜ k should be chosen to balance local and global structure. Cross-validation and domain knowledge can help determine the best value.\n",
        "\n",
        "When Isomap is Superior to PCA Isomap is superior to PCA when the data lies on a non-linear manifold. Specifically:\n",
        "\n",
        "PCA assumes linearity and works well when the data is distributed around a straight hyperplane. Isomap captures non-linear relationships by using geodesic distances, making it more suitable for curved or non-linear manifolds (e.g., Swiss roll datasets). Other Manifold Learning Methods Here are some alternatives to Isomap and their key differences:\n",
        "\n",
        "Locally Linear Embedding (LLE):\n",
        "\n",
        "Focuses on preserving local linear relationships between points. Constructs a local neighborhood for each point and minimizes reconstruction errors. Difference: LLE emphasizes local relationships without explicitly considering global geodesic distances. t-Distributed Stochastic Neighbor Embedding (t-SNE):\n",
        "\n",
        "Converts high-dimensional data into probabilities that represent pairwise similarities, preserving local structure. Optimized for visualization, typically in 2D or 3D. Difference: t-SNE is probabilistic and focuses primarily on local structure, often losing global relationships. UMAP (Uniform Manifold Approximation and Projection):\n",
        "\n",
        "A modern alternative to t-SNE that preserves both local and global structures better. Computationally faster and more scalable than t-SNE. Difference: UMAP uses a graph-theoretic approach to model local and global topologies. Self-Organizing Maps (SOM):\n",
        "\n",
        "Uses a neural network-like approach to project data onto a lower-dimensional grid. Useful for clustering and visualization tasks. Difference: SOM is more clustering-oriented and not primarily focused on maintaining manifold geometry. Dealing with Missing Data in Manifold Learning Manifold learning methods are sensitive to missing data. Here are ways to address this issue:\n",
        "\n",
        "Imputation:\n",
        "\n",
        "Fill missing values using techniques like mean, median, or k-nearest neighbor imputation. Use advanced methods like multiple imputations or matrix factorization for better accuracy. Sparse Algorithms:\n",
        "\n",
        "Some algorithms (e.g., sparse PCA) are designed to handle missing data directly. Modified Distance Metrics:\n",
        "\n",
        "Use modified distance metrics (e.g., pairwise distance ignoring missing entries) for constructing neighborhood graphs. Preprocessing:\n",
        "\n",
        "Remove data points or features with excessive missing values, provided it doesnâ€™t compromise manifold integrity. Iterative Manifold Reconstruction:\n",
        "\n",
        "Use iterative methods where the manifold is reconstructed while imputing missing values based on neighbors.\n",
        "\n",
        "Some interesting references:\n",
        "1) https://axon.cs.byu.edu/Dan/678/miscellaneous/Manifold.example.pdf\n",
        "\n",
        "2) http://vision.cse.psu.edu/seminars/talks/PRML/David_NDR_lecture.pdf\n",
        "\n",
        "Colab paid products - Cancel contracts here\n"
      ]
    }
  ]
}